{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c634d9da-c64e-46f8-b558-ff8ca345221b",
      "metadata": {
        "id": "c634d9da-c64e-46f8-b558-ff8ca345221b"
      },
      "source": [
        "# Transformers From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f32337e-e558-4902-b6bc-e1487a29565b",
      "metadata": {
        "id": "5f32337e-e558-4902-b6bc-e1487a29565b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import math\n",
        "import scipy\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158f0264-eb1f-4b8d-b899-9c9e78ad0b69",
      "metadata": {
        "tags": [],
        "id": "158f0264-eb1f-4b8d-b899-9c9e78ad0b69"
      },
      "source": [
        "## 1. Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a68ae2-a326-4a4b-8b00-81c0fad35f5e",
      "metadata": {
        "id": "60a68ae2-a326-4a4b-8b00-81c0fad35f5e"
      },
      "source": [
        "Byte Pair Encoding (BPE) is a widely used method for constructing efficient vocabularies in natural language processing models. It addresses several challenges, such as the need for compact vocabularies, handling rare or unknown words, and maintaining semantic coherence.\n",
        "\n",
        "The key idea behind BPE is to break words down into characters initially and then iteratively merge the most frequently occurring pairs of characters or subwords. This process continues until a predefined vocabulary size is reached. The result is a set of subword units that can effectively represent both common and rare words.\n",
        "\n",
        "For example, instead of storing \"unhappiness\" as a single word, BPE might break it into \"un,\" \"happi,\" and \"ness.\" This allows the model to recombine these subword units to understand or generate related words like \"happiness\" or \"unhappy.\"\n",
        "\n",
        "BPE has been instrumental in enabling models like GPT to train efficiently on massive datasets while maintaining a vocabulary size (e.g., 50,257 tokens for GPT) that balances computational efficiency and linguistic expressiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def8cfed-c86d-42c7-bebe-17df8fae8b4f",
      "metadata": {
        "id": "def8cfed-c86d-42c7-bebe-17df8fae8b4f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Build vocabulary by separating characters in words and adding an end-of-token marker.\n",
        "def build_vocab(corpus):\n",
        "    # Split each word into characters, join them with spaces, and add the end-of-word marker \"</w>\".\n",
        "    tokens = [\" \".join(word) + \" </w>\" for word in corpus.split()]\n",
        "    return Counter(tokens)  # Count the frequency of each token\n",
        "\n",
        "# Get counts of pairs of consecutive symbols in the vocabulary.\n",
        "def get_stats(vocab):\n",
        "    pairs = Counter()\n",
        "    for word, frequency in vocab.items():\n",
        "        symbols = word.split()\n",
        "        # Count occurrences of consecutive symbol pairs.\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += frequency\n",
        "    return pairs\n",
        "\n",
        "# Merge the most frequent pairs in the vocabulary.\n",
        "def merge_vocab(pair, vocab):\n",
        "    merged_vocab = Counter()\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(rf\"(?<!\\S){bigram}(?!\\S)\")  # Regex pattern to find whole bigrams\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        # Replace the bigram in the word with the merged version.\n",
        "        merged_word = pattern.sub(\"\".join(pair), word)\n",
        "        merged_vocab[merged_word] = freq\n",
        "\n",
        "    return merged_vocab\n",
        "\n",
        "# Tokenize text into subword units based on sorted tokens and input mappings.\n",
        "def tokenize(\n",
        "    text,\n",
        "    sorted_tokens,\n",
        "    input_id_map,\n",
        "    return_strings=False,\n",
        "    max_length=32,\n",
        "    unknown_token=\"</u>\"\n",
        "):\n",
        "    # Append the end-of-word marker \"</w>\" to each word in the input text.\n",
        "    text = \" \".join([word + \"</w>\" for word in text.split()])\n",
        "\n",
        "    def recursive_tokenize(text, sorted_tokens):\n",
        "        if not text:\n",
        "            # If the input text is empty, return an empty list.\n",
        "            return []\n",
        "        if not sorted_tokens:\n",
        "            # If there are no sorted tokens left, return the unknown token.\n",
        "            return [unknown_token]\n",
        "\n",
        "        tokens = []\n",
        "        for i, token in enumerate(sorted_tokens):\n",
        "            token_pattern = re.escape(token)  # Escape special characters in the token.\n",
        "            matches = [(m.start(), m.end()) for m in re.finditer(token_pattern, text)]\n",
        "\n",
        "            if not matches:\n",
        "                # Skip to the next token if there are no matches.\n",
        "                continue\n",
        "\n",
        "            end_positions = [start for start, _ in matches]  # Extract start positions of matches.\n",
        "            start_position = 0\n",
        "\n",
        "            for end_position in end_positions:\n",
        "                # Tokenize the substring before the match recursively.\n",
        "                substring = text[start_position:end_position]\n",
        "                tokens += recursive_tokenize(substring, sorted_tokens[i + 1:])\n",
        "                tokens.append(token)  # Add the matched token to the tokens list.\n",
        "                start_position = end_position + len(token)\n",
        "\n",
        "            # Tokenize any remaining part of the string after the matches.\n",
        "            remaining_text = text[start_position:]\n",
        "            tokens += recursive_tokenize(remaining_text, sorted_tokens[i + 1:])\n",
        "            break\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    tokenized = recursive_tokenize(text, sorted_tokens)\n",
        "\n",
        "    if return_strings:\n",
        "        # Return tokens as strings if requested.\n",
        "        return tokenized\n",
        "\n",
        "    input_ids = [1] * max_length  # Initialize input IDs with a default value.\n",
        "    attention_mask = [0] * max_length  # Initialize the attention mask.\n",
        "\n",
        "    for i, token in enumerate(tokenized[:max_length]):\n",
        "        # Map tokens to input IDs, handling unknown tokens gracefully.\n",
        "        input_ids[i] = input_id_map.get(token, input_id_map.get(unknown_token, 0))\n",
        "        attention_mask[i] = 1  # Set attention mask for valid tokens.\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "# corpus = \"this is a sample corpus\"\n",
        "# vocab = build_vocab(corpus)\n",
        "# print(vocab)\n",
        "# sorted_tokens = [\"a\", \"sample\", \"is\", \"this\", \" </w>\"]\n",
        "# input_id_map = {token: i for i, token in enumerate(sorted_tokens)}\n",
        "# print(tokenize(\"this is a sample\", sorted_tokens, input_id_map))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d0942f-5208-4434-837e-7b5f99bd3d35",
      "metadata": {
        "id": "d9d0942f-5208-4434-837e-7b5f99bd3d35"
      },
      "outputs": [],
      "source": [
        "def get_tokens_from_vocab(vocab):\n",
        "    \"\"\"\n",
        "    Extract tokens and their frequencies from the vocabulary.\n",
        "    Also create a mapping of tokenized words to their original words.\n",
        "\n",
        "    Args:\n",
        "        vocab (Counter): Vocabulary with words and their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        tokens_frequencies (Counter): Frequency of individual tokens.\n",
        "        vocab_tokenization (dict): Mapping of concatenated tokens to tokenized form.\n",
        "    \"\"\"\n",
        "    tokens_frequencies = Counter()\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "# Create a mapping of tokens to integers and back.\n",
        "def measure_token_length(token):\n",
        "    \"\"\"\n",
        "    Measure the length of a token. For tokens ending with \"</w>\",\n",
        "    subtract the marker length and add 1.\n",
        "\n",
        "    Args:\n",
        "        token (str): Input token.\n",
        "\n",
        "    Returns:\n",
        "        int: Measured length of the token.\n",
        "    \"\"\"\n",
        "    return len(token[:-4]) + 1 if token[-4:] == '</w>' else len(token)\n",
        "\n",
        "# Sort tokens by their length (adjusted for end markers) and frequency in descending order.\n",
        "sorted_tokens = [\n",
        "    token\n",
        "    for (token, freq) in sorted(\n",
        "        tokens_frequencies.items(),\n",
        "        key=lambda item: (measure_token_length(item[0]), item[1]),\n",
        "        reverse=True,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Define special tokens for unknown and padding.\n",
        "UNK_TOKEN = '</u>'\n",
        "PAD_TOKEN = '</p>'\n",
        "\n",
        "# Map tokens to unique integers, starting with special tokens.\n",
        "token2id = {UNK_TOKEN: 0, PAD_TOKEN: 1}\n",
        "for i, token in enumerate(sorted_tokens):\n",
        "    token2id[token] = i + 2\n",
        "\n",
        "# Create a reverse mapping from integers to tokens.\n",
        "id2token = {v: k for k, v in token2id.items()}\n",
        "\n",
        "# Embedding initialization:\n",
        "# To enable models to learn semantic meaning dynamically, we map tokens to random vectors.\n",
        "# The dimension of these vectors can vary based on the application's requirements.\n",
        "# Larger dimensions encode more information but require greater computational resources.\n",
        "\n",
        "# Initialize embeddings as random vectors.\n",
        "# embedding_dim = 128\n",
        "# embeddings = {token: np.random.rand(embedding_dim) for token in token2id}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46dc5f6-bb32-4937-8305-45b86e5b12ec",
      "metadata": {
        "id": "c46dc5f6-bb32-4937-8305-45b86e5b12ec",
        "outputId": "c872c34f-bae2-4820-ed63-cf66f53a9a00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3815, 32])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_dim = 32\n",
        "token_embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "token_embeddings.weight.size() # vocab_size x embedding_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0edda2c-12dd-4e2d-942b-6073b7d5de6b",
      "metadata": {
        "tags": [],
        "id": "f0edda2c-12dd-4e2d-942b-6073b7d5de6b"
      },
      "source": [
        "## 2. Positional Encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "259fae64-79f9-43c6-a85b-a7a3888efeac",
      "metadata": {
        "id": "259fae64-79f9-43c6-a85b-a7a3888efeac"
      },
      "source": [
        "# Positional Encoding\n",
        "\n",
        "Positional encoding is a method used in Transformer models to encode the position of tokens in a sequence, enabling the model to understand their order. Unlike RNNs, which inherently capture sequential information through recurrence, Transformers rely on positional encoding since they lack this structural feature.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Positional Encoding?\n",
        "- Transformers process tokens in parallel and do not have built-in sequential mechanisms.\n",
        "- Positional encoding adds information about the order of tokens directly into the input embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Properties of Positional Encoding\n",
        "1. **Uniqueness**: Each position has a distinct encoding.\n",
        "2. **Consistency**: Relative distances between positions remain consistent across sequences of varying lengths.\n",
        "3. **Generality**: The encoding generalizes well to longer sequences.\n",
        "4. **Determinism**: The process is fixed and reproducible.\n",
        "\n",
        "---\n",
        "\n",
        "## Mathematical Definition\n",
        "\n",
        "The positional encoding is represented as a $d$-dimensional vector added to the token embeddings. It is defined as:\n",
        "\n",
        "\\[\n",
        "PE_{(pos, i)} =\n",
        "\\begin{cases}\n",
        "\\sin\\left(\\frac{pos}{10000^{\\frac{i}{d_{model}}}}\\right) & \\text{if } i \\bmod 2 = 0 \\\\\n",
        "\\cos\\left(\\frac{pos}{10000^{\\frac{i}{d_{model}}}}\\right) & \\text{if } i \\bmod 2 \\neq 0\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( pos \\): Position of the token in the sequence.\n",
        "- \\( i \\): Dimension index of the embedding.\n",
        "- \\( d_{model} \\): Dimensionality of the model.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Sine and Cosine?\n",
        "- The periodic nature allows encodings to generalize to unseen positions.\n",
        "- Both sine and cosine functions create unique yet consistent encodings.\n",
        "- These functions enable the model to compute relative distances between positions efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## How It's Used\n",
        "- The positional encoding vector is added to the token embedding vector for each position.\n",
        "- This enriched representation is fed into the Transformer layers, allowing the model to process both content and positional information.\n",
        "\n",
        "By using positional encoding, Transformers effectively handle the sequential nature of language, enabling them to excel in tasks like translation, summarization, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785e22e9-ecf1-4e40-a5ff-35b0478ca19e",
      "metadata": {
        "id": "785e22e9-ecf1-4e40-a5ff-35b0478ca19e"
      },
      "source": [
        "Lets create matrix of `[SeqLen, HiddenDim]` representing the positional encoding for `max_len` inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f77313-d1f2-40be-8ad6-1937ab8222ca",
      "metadata": {
        "id": "50f77313-d1f2-40be-8ad6-1937ab8222ca",
        "outputId": "25bb8bdc-7ca4-4998-9fdd-e99e01843685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 32])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def position_encode(max_len, emb_dim):\n",
        "    \"\"\"\n",
        "    Generate positional encodings for sequences.\n",
        "\n",
        "    Args:\n",
        "        max_len (int): Maximum sequence length.\n",
        "        emb_dim (int): Embedding dimension.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of size (max_len, emb_dim) containing positional encodings.\n",
        "    \"\"\"\n",
        "    # Initialize a tensor of zeros for positional encodings.\n",
        "    pe = torch.zeros(max_len, emb_dim)\n",
        "\n",
        "    # Create a column vector of position indices [0, 1, ..., max_len-1].\n",
        "    position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    # Calculate the scaling factor for even dimensions.\n",
        "    div_term = torch.exp(-math.log(10000.0) * torch.arange(0, emb_dim, 2).float() / emb_dim)\n",
        "\n",
        "    # Compute sine for even indices and cosine for odd indices.\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even dimensions.\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd dimensions.\n",
        "\n",
        "    return pe\n",
        "\n",
        "\n",
        "encoded_positions = position_encode(64, 32)\n",
        "print(encoded_positions.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb52236a-d94d-4263-8927-a82f91fb3309",
      "metadata": {
        "id": "fb52236a-d94d-4263-8927-a82f91fb3309"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, max_len, sinusoidal=True):\n",
        "        \"\"\"\n",
        "        Initialize the PositionalEncoding module.\n",
        "\n",
        "        Args:\n",
        "            emb_dim (int): Hidden dimensionality of the input.\n",
        "            max_len (int): Maximum length of a sequence to expect.\n",
        "            sinusoidal (bool): Whether to use sinusoidal positional encodings or learned embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sinusoidal = sinusoidal\n",
        "        if self.sinusoidal:\n",
        "            # Generate sinusoidal positional encodings.\n",
        "            pe = position_encode(max_len, emb_dim).unsqueeze(0)  # Add batch dimension.\n",
        "            self.register_buffer('pe', pe, persistent=False)  # Register buffer to avoid optimization.\n",
        "        else:\n",
        "            # Use learned positional embeddings.\n",
        "            self.register_buffer('pos', torch.arange(0, max_len, dtype=torch.long).unsqueeze(0))\n",
        "            self.pe = nn.Embedding(max_len, emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encodings to the input.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, emb_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor with positional encodings added.\n",
        "        \"\"\"\n",
        "        if self.sinusoidal:\n",
        "            return x + self.pe[:, :x.size(1)]  # Add sinusoidal encodings.\n",
        "        else:\n",
        "            return x + self.pe(self.pos[:, :x.size(1)])  # Add learned embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914ca37f-a7b2-47cc-9fe9-7b062f974677",
      "metadata": {
        "tags": [],
        "id": "914ca37f-a7b2-47cc-9fe9-7b062f974677"
      },
      "source": [
        "## 3. Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2656c2-c8fa-411f-ba05-6eab23cd392f",
      "metadata": {
        "id": "cc2656c2-c8fa-411f-ba05-6eab23cd392f"
      },
      "source": [
        "# Attention Mechanism\n",
        "\n",
        "The attention mechanism answers the question:  \n",
        "**How relevant is each element in a sequence to the others?**\n",
        "\n",
        "It allows models to dynamically focus on the most relevant parts of the input when generating output, rather than relying on a single global representation.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "Each element in the sequence is represented by three vectors: **Query (Q)**, **Key (K)**, and **Value (V)**.\n",
        "\n",
        "- **Query (Q):** Describes what a token is looking for in other tokens (what it wants to attend to).  \n",
        "- **Key (K):** Represents what a token offers and when it might be important with respect to the query.  \n",
        "- **Value (V):** Contains the actual information of the token, used to compute the output.\n",
        "\n",
        "---\n",
        "\n",
        "## Attention Computation\n",
        "\n",
        "The attention mechanism computes a weighted combination of the values \\(V\\), where the weights are based on the similarity between the query \\(Q\\) and the keys \\(K\\). This is defined as:\n",
        "\n",
        "$$\n",
        "\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "### Breakdown:\n",
        "1. $$Q K^T$$: Calculates the similarity between queries and keys.  \n",
        "2. $$\\sqrt{d_k}$$: Scales the dot product to avoid large values that hinder training.  \n",
        "3. $$\\operatorname{softmax}$$: Converts similarity scores into probabilities.  \n",
        "4. Weighted sum of \\(V\\): Produces the output representation for each token.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Attention Matters\n",
        "- **Dynamic Focus:** The model identifies and emphasizes the most relevant parts of the input.  \n",
        "- **Context Sensitivity:** Enables better understanding of relationships within the sequence.  \n",
        "- **Scalability:** Forms the foundation of modern architectures like Transformers, facilitating parallel processing and high efficiency.\n",
        "\n",
        "The attention mechanism is key to enabling models to process sequences effectively and has revolutionized tasks in NLP, vision, and beyond.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a98449f-f6a6-4ee2-ba14-4b7c26336acd",
      "metadata": {
        "id": "4a98449f-f6a6-4ee2-ba14-4b7c26336acd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Scaled dot-product attention function\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        q (torch.Tensor): Query tensor of shape (..., seq_len_q, depth).\n",
        "        k (torch.Tensor): Key tensor of shape (..., seq_len_k, depth).\n",
        "        v (torch.Tensor): Value tensor of shape (..., seq_len_k, depth_v).\n",
        "        mask (torch.Tensor, optional): Mask tensor of shape (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output values after applying attention.\n",
        "        torch.Tensor: Attention weights.\n",
        "    \"\"\"\n",
        "    # Compute attention logits by multiplying query and key transpose.\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    # Scale the logits by the square root of the depth.\n",
        "    attn_logits = attn_logits / math.sqrt(q.size()[-1])\n",
        "\n",
        "    if mask is not None:\n",
        "        # Apply mask to prevent attention on certain positions.\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "\n",
        "    # Compute attention weights using softmax.\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    # Compute the weighted sum of values.\n",
        "    values = torch.matmul(attention, v)\n",
        "\n",
        "    return values, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d3de22d-74c4-44f0-aaa2-9edbdb8e4028",
      "metadata": {
        "id": "4d3de22d-74c4-44f0-aaa2-9edbdb8e4028",
        "outputId": "02575393-c584-4838-817f-731ee05fa0ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 64]), torch.Size([64, 64]))"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=torch.Tensor(attention_mask).long())\n",
        "values.size(), attention.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878ec489-956b-4c87-afd6-cb0463966a20",
      "metadata": {
        "tags": [],
        "id": "878ec489-956b-4c87-afd6-cb0463966a20"
      },
      "source": [
        "### Multi-head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b084bf7-a348-4872-b010-54344d3fc7b2",
      "metadata": {
        "id": "1b084bf7-a348-4872-b010-54344d3fc7b2"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "**Multi-Head Attention** is an extension of the attention mechanism that allows a model to focus on different parts of the sequence simultaneously. It is a critical component of Transformer architectures.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Multi-Head Attention?\n",
        "1. **Diverse Representations:** A single attention mechanism might focus on only one aspect of the relationships in a sequence. Multi-head attention enables the model to capture different types of relationships in parallel.\n",
        "2. **Improved Learning Capacity:** By using multiple \"heads,\" the model can learn to attend to different positions and features in the input sequence.\n",
        "3. **Efficient Representation:** Combines outputs from multiple attention mechanisms into a richer and more expressive representation.\n",
        "\n",
        "---\n",
        "\n",
        "## Formula\n",
        "The output of multi-head attention is:\n",
        "\\[\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(\\text{head}_i = \\operatorname{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\)\n",
        "- \\(W_i^Q, W_i^K, W_i^V\\): Projection matrices for the \\(i\\)-th head.\n",
        "- \\(W^O\\): Projection matrix to combine all heads.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Benefits\n",
        "1. **Parallelism:** Multiple heads work in parallel, improving efficiency.\n",
        "2. **Enhanced Context:** Each head focuses on different relationships in the sequence.\n",
        "3. **Flexibility:** Allows the model to capture nuanced patterns in data.\n",
        "\n",
        "---\n",
        "\n",
        "## Applications\n",
        "- Multi-head attention is a core part of the **Transformer architecture** and is widely used in tasks like:\n",
        "  - Machine Translation\n",
        "  - Text Summarization\n",
        "  - Question Answering\n",
        "  - Vision Transformers (ViT) in Computer Vision\n",
        "\n",
        "Multi-head attention enhances the model's ability to process complex sequences and is a cornerstone of modern deep learning architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ec6c46-8ae8-4f95-85b3-956aef419eb6",
      "metadata": {
        "id": "51ec6c46-8ae8-4f95-85b3-956aef419eb6"
      },
      "source": [
        "The following is the full implementation of multi-head attention as a PyTorch module."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multi-head attention module\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, emb_dim, num_heads, max_len):\n",
        "        \"\"\"\n",
        "        Initialize the MultiheadAttention module.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of input features.\n",
        "            emb_dim (int): Dimensionality of the embedding space.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            max_len (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert emb_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_dim // num_heads\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3 * emb_dim)\n",
        "        self.o_proj = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "        # Causal mask for autoregressive attention\n",
        "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len))\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        \"\"\"Initialize parameters using Xavier initialization.\"\"\"\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        \"\"\"\n",
        "        Forward pass for multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n",
        "            mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
        "            return_attention (bool, optional): Whether to return attention weights. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying multi-head attention.\n",
        "            torch.Tensor (optional): Attention weights (if return_attention is True).\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from the linear projection output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, 3 * HeadDim]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Use causal mask if no mask is provided\n",
        "        if mask is None:\n",
        "            mask = self.causal_mask[:, :, :seq_length, :seq_length].expand(batch_size, self.num_heads, -1, -1)\n",
        "        else:\n",
        "            mask = mask.view(batch_size, 1, 1, seq_length).expand(-1, self.num_heads, seq_length, -1)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Concatenate attention outputs\n",
        "        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, HeadDim]\n",
        "        values = values.reshape(batch_size, seq_length, self.emb_dim)\n",
        "\n",
        "        # Final linear projection\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n"
      ],
      "metadata": {
        "id": "WB3rfv5ewo4m"
      },
      "id": "WB3rfv5ewo4m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "be97b7eb-854d-4d0a-8c00-fb7b22c67b24",
      "metadata": {
        "tags": [],
        "id": "be97b7eb-854d-4d0a-8c00-fb7b22c67b24"
      },
      "source": [
        "## 4. Transformer Block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a3fddf-ee1a-4ccb-b645-cc0b3f9c3efc",
      "metadata": {
        "id": "14a3fddf-ee1a-4ccb-b645-cc0b3f9c3efc"
      },
      "source": [
        "# Transformer Block\n",
        "\n",
        "The **Transformer Block** is the core unit of the Transformer architecture, designed to process sequences by focusing on token relationships without recurrence or convolution.\n",
        "\n",
        "---\n",
        "\n",
        "## Components\n",
        "\n",
        "1. **Multi-Head Attention**  \n",
        "   - Allows the model to focus on different parts of the sequence simultaneously.  \n",
        "   - Captures diverse relationships and patterns across tokens.\n",
        "\n",
        "2. **Feed-Forward Network**  \n",
        "   - Enhances representations through non-linear transformations.  \n",
        "   - Applied independently to each token.\n",
        "\n",
        "3. **Residual Connections and Layer Normalization**  \n",
        "   - Stabilize training and preserve original information.  \n",
        "   - Residual connections add inputs back to outputs; normalization improves convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow\n",
        "1. **Input Embeddings with Positional Encoding**  \n",
        "2. **Multi-Head Attention**  \n",
        "3. **Add & Normalize**  \n",
        "4. **Feed-Forward Network**  \n",
        "5. **Add & Normalize**  \n",
        "6. **Output**: Refined token representations.\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages\n",
        "- **Parallel Processing:** Faster than RNNs.  \n",
        "- **Long-Range Dependencies:** Captures relationships between distant tokens.  \n",
        "- **Scalability:** Adapts to various tasks by stacking multiple blocks.\n",
        "\n",
        "---\n",
        "\n",
        "## Applications\n",
        "Used in models like **BERT**, **GPT**, and **Vision Transformers**, enabling state-of-the-art performance across NLP and vision tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64eae47c-6a4d-4df4-b43a-1ff2d1643f72",
      "metadata": {
        "id": "64eae47c-6a4d-4df4-b43a-1ff2d1643f72"
      },
      "source": [
        "### Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7b7a28-25c4-4d21-b25a-0ede1f2466d3",
      "metadata": {
        "id": "6c7b7a28-25c4-4d21-b25a-0ede1f2466d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Layer normalization function\n",
        "def layer_normalization(x, gamma, beta):\n",
        "    \"\"\"\n",
        "    Perform layer normalization.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor of shape (..., dim).\n",
        "        gamma (torch.Tensor): Scale parameter of shape (..., dim).\n",
        "        beta (torch.Tensor): Shift parameter of shape (..., dim).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Normalized tensor.\n",
        "    \"\"\"\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
        "    std = (var + 1e-9).sqrt()\n",
        "    y = (x - mean) / std\n",
        "    y = y * gamma\n",
        "    y = y + beta\n",
        "    return y\n",
        "\n",
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, model_dim, max_len, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer block.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of the input.\n",
        "            num_heads (int): Number of heads to use in the attention.\n",
        "            model_dim (int): Dimensionality of the hidden layer in the MLP.\n",
        "            max_len (int): Maximum length of the input sequence.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            input_dim=input_dim, emb_dim=input_dim, num_heads=num_heads, max_len=max_len\n",
        "        )\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, model_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(model_dim, input_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the Transformer block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_dim).\n",
        "            mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after processing.\n",
        "        \"\"\"\n",
        "        x = x + self.self_attn(self.norm1(x), mask=mask)\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2accc7ce-f145-4a48-90cc-d020bcff2077",
      "metadata": {
        "tags": [],
        "id": "2accc7ce-f145-4a48-90cc-d020bcff2077"
      },
      "source": [
        "## 5. Transformer Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df9a71-88e0-446c-a147-c06c73723e17",
      "metadata": {
        "id": "b6df9a71-88e0-446c-a147-c06c73723e17"
      },
      "source": [
        "We can now combine all of the ingredients i.e. BPE, token embeddings, positional encoding, self-attention and Transformer blocks to create the Transformer architecture. We can stack multiple Transformer blocks to scale the expressive power of our models.\n",
        "\n",
        "Generally, we can allow the attention layers to attend to the entire sequence i.e. both the previous and the next words. This setting can be considered as an \"encoder\", as the model is learning to encode the entire sequence into something meaningful.\n",
        "\n",
        "If we limit the attention to only one direction with masking, we call this setting as a \"decoder\", as the model is learning to look in the past and generate (decode) the next most-likely token.\n",
        "\n",
        "BERT like models use encoder architecture and language models like GPT use decoder architecture. Both of these settings can be combined as well. Machine Translation models generally use encoder-decoder architecture (as in the original 2017 paper).\n",
        "\n",
        "![image.png](attachment:6e762df2-93b3-4814-bf9b-363760af964a.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482baeb6-cf02-4cc1-a20b-b95d796424b2",
      "metadata": {
        "id": "482baeb6-cf02-4cc1-a20b-b95d796424b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Transformer model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        max_len,\n",
        "        emb_dim,\n",
        "        model_dim,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        dropout=0.1,\n",
        "        position_sinusoidal=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            max_len (int): Maximum sequence length.\n",
        "            emb_dim (int): Dimensionality of token embeddings.\n",
        "            model_dim (int): Dimensionality of the model layers.\n",
        "            num_layers (int): Number of Transformer blocks.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            dropout (float): Dropout probability.\n",
        "            position_sinusoidal (bool): Use sinusoidal positional encodings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.input_projection = nn.Linear(emb_dim, model_dim)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            model_dim, max_len, sinusoidal=position_sinusoidal\n",
        "        )\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(model_dim, num_heads, model_dim, max_len, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.output_projection = nn.Linear(model_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, return_logits=True):\n",
        "        \"\"\"\n",
        "        Forward pass for the Transformer model.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
        "            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
        "            return_logits (bool, optional): Return logits if True, else return hidden states. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Model output logits or hidden states.\n",
        "        \"\"\"\n",
        "        embedded_tokens = self.embedding(input_ids)\n",
        "        projected_inputs = self.input_projection(embedded_tokens)\n",
        "        encoded_inputs = self.positional_encoding(projected_inputs)\n",
        "\n",
        "        x = encoded_inputs\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask=attention_mask)\n",
        "\n",
        "        output = self.output_projection(x) if return_logits else x\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3cbda4-6ad4-4a83-a1f8-630f40c65e72",
      "metadata": {
        "id": "ba3cbda4-6ad4-4a83-a1f8-630f40c65e72"
      },
      "outputs": [],
      "source": [
        "#With causual-attention and by adjusting the `num_layers`, `num_heads` and `model_dim`, most GPT models can be implemented with our code.\n",
        "gpt2 = dict(num_layers=12, num_heads=12, model_dim=768)  # 120M params\n",
        "gpt3 = dict(num_layers=96, num_heads=96, model_dim=2048)  # 175B params\n",
        "gpt_mini = dict(num_layers=6, num_heads=6, model_dim=192) # 1.2M params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37cc07f4-fc1b-48e7-926b-16804537bd81",
      "metadata": {
        "id": "37cc07f4-fc1b-48e7-926b-16804537bd81"
      },
      "outputs": [],
      "source": [
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_len,\n",
        "    emb_dim=32,\n",
        "    **gpt_mini\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f45fae1-7e5b-4ce7-962b-d6f1018a454c",
      "metadata": {
        "id": "3f45fae1-7e5b-4ce7-962b-d6f1018a454c"
      },
      "outputs": [],
      "source": [
        "input_text = 'The fox jumps over the fence'\n",
        "\n",
        "input_ids, attention_mask = tokenize(input_text,\n",
        "                     sorted_tokens=sorted_tokens,\n",
        "                     input_id_map=token2id,\n",
        "                     return_strings=False,\n",
        "                     max_length=max_len,\n",
        "                     unknown_token='</u>')\n",
        "\n",
        "input_ids = torch.Tensor([input_ids]).long()\n",
        "attention_mask = torch.Tensor([attention_mask]).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02dc756-e788-4fbc-af3a-660d7e99f069",
      "metadata": {
        "id": "d02dc756-e788-4fbc-af3a-660d7e99f069",
        "outputId": "ecd0059c-7968-4572-df9e-40cc6c728029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 3815])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(input_ids, attention_mask).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85ec73d-76c5-4e4c-b7a9-f1a4364943bd",
      "metadata": {
        "tags": [],
        "id": "a85ec73d-76c5-4e4c-b7a9-f1a4364943bd"
      },
      "source": [
        "## 6. Transformer Training - Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90cc1695-b638-465b-84bd-5d980ad0255f",
      "metadata": {
        "id": "90cc1695-b638-465b-84bd-5d980ad0255f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Example dataset class\n",
        "def prepare_dataset(tokenizer, texts, max_len):\n",
        "    \"\"\"Tokenize texts and create attention masks.\"\"\"\n",
        "    tokenized_inputs = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        tokens = tokenizer(text)\n",
        "        if len(tokens) > max_len:\n",
        "            tokens = tokens[:max_len]\n",
        "        attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))\n",
        "        tokens += [0] * (max_len - len(tokens))\n",
        "        tokenized_inputs.append(tokens)\n",
        "        attention_masks.append(attention_mask)\n",
        "    return torch.tensor(tokenized_inputs), torch.tensor(attention_masks)\n",
        "\n",
        "class ExampleDataset(Dataset):\n",
        "    def __init__(self, inputs, masks, labels):\n",
        "        self.inputs = inputs\n",
        "        self.masks = masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.masks[idx], self.labels[idx]\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, masks, labels in dataloader:\n",
        "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, masks)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, masks, labels in dataloader:\n",
        "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, masks)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Hyperparameters and setup\n",
        "vocab_size = 30522\n",
        "max_len = 128\n",
        "emb_dim = 256\n",
        "model_dim = 256\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "dropout = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model\n",
        "tokenizer = lambda text: [ord(c) for c in text]  # Dummy tokenizer\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_len,\n",
        "    emb_dim=emb_dim,\n",
        "    model_dim=model_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "texts = [\"hello world\", \"transformers are powerful\"] * 100\n",
        "labels = [[1] + [0] * (max_len - 1)] * len(texts)  # Dummy labels\n",
        "tokenized_inputs, attention_masks = prepare_dataset(tokenizer, texts, max_len)\n",
        "dataset = ExampleDataset(tokenized_inputs, attention_masks, torch.tensor(labels))\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_model(model, dataloader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82a9cc7-b227-4627-af80-767a50f16e44",
      "metadata": {
        "id": "e82a9cc7-b227-4627-af80-767a50f16e44",
        "outputId": "7bca7526-595a-4e6a-fa63-ca358d3eb3cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([   12,   437, 12782,  ...,  3873, 46664,    11]),\n",
              " tensor([  437, 12782,   815,  ..., 46664,    11,   284]),\n",
              " tensor([1, 1, 1,  ..., 1, 1, 1]))"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x, y, mask = train_dataset[100]\n",
        "x, y, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b63795-896a-4017-beab-db2381e82b8c",
      "metadata": {
        "tags": [],
        "id": "d7b63795-896a-4017-beab-db2381e82b8c"
      },
      "source": [
        "## References\n",
        "[1] https://huggingface.co/blog/transformers\n",
        "[2] https://paperswithcode.com/method/transformer\n",
        "[3] https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "morpheus-cu116",
      "language": "python",
      "name": "morpheus-cu116"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}